###### **Тема 1. Задачи машинного обучения**
`Классическое машинное обучение`

Задачи машинного обучения условно разделены на :
1. Задачи обучения с учителем:
* Классификация
* Регрессия
2. Задачи обучения без учителя:
* Задачи кластеризации
* Задачи Ассоциации
* Задачи поисков аномалий
* Задачи уменьшения размерности данных


###### **Тема 2. Модель и процесс машинного обучения**
Для рассмотрения процесса подготовки данных возьмем файл demo13.xls
Прежде загрузим билиотеки
```python
import pandas as pd
import numpy as np
```
Теперь получим данные из файла
```python
xls = pd.ExcelFile('demo13.xls')
```
Возьмем один лист файла
```python
sheet = xls.parse(0)
```
Теперь возьмем только строки с 6 по 34 , 3 столбец из которого возьмем только мужчин, отбросим индексы и преобразуем в серию данных (Dataset)
```python
data = pd.DataFrame({'Мужчины': sheet['Unnamed:2'][6:34].reset_index{}['Unnamed:2']})
```
Построим модель
```python
model =float(data.mean())
```
Выведим результат
```python
print('{0:.4}'.format(model))
```
Расчитаем точность модели
```python
err = np.abs(data - model).mean()
```
Выведем результат
```python
print('{0:.3}%'.format(float(100*err/model)))
```
Применим модель и расчитаем ошибку исходя из того что по данным росстата количесвто мужчин в 2021 году составляла 67.85 млн.
```python
result = 67.85
print('{0:.3}%'.format(float(100*(result-model)/result)))
```
Сделаем такой же расчет для женщин. Дата Фрайм
```python
data_w = pd.DataFrame({'Женщины': sheet['Unnamed: 3'][6:34].reset_index()['Unnamed: 3']})
```
Модель
```python
model_w = float(data_w).mean()
```
Ошибка
```python
err_w = np.abs(data_w-model_w).mean()
```
Выведем ошибку и среденее для женщин
```python
print('{0:.3}%'.format(float(100*err_w/model_w)))
print('{0:.4}'.format(model_w))
```
Применим модель и высчитаем ошибку для 2021 года (женщин 78.32)
```python
result_w = 78.32
print('{0:.3}%'.format(float(100*(result_w-model_w)/result_w)))
```























###### **Тема 3. Потоки данных в машинном обучении**
###### **Тема 4. Задача регрессии**
Для задачи регрессии расчета потребелния электроэнергии здания используем три набора данных
* buildung_metadata.csv
* train.csv
* weather.csv
Загрузим необходимые библиотеки
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from matploylib.pyplot import rcParams
rcParams['figure', 'figsize'] = 16, 8
```
теперь загрузим датафрейм
```python
data = pd.read_csv('train.csv')
```
Отрисуем график 
```python
data.set_index('timestamp')['meter_reading'].plot()
plt.show()
```
Построим модель вычисления среднего
```python
model = float(data['meter_reading'].meean())
```
выведем занчение
```python
print('{0:.4}'.format(model))
```
Ошибка составит
```python
err = np.abs(data['meter_reading']-model).mean()
```
Вывод значения
```python
print('{0:.3}%'.format(float(100*err/model)))
```

###### **Тема 5. Разведочный (исследовательский) анализ данных EDA**
Прежде всего загрузим необходимые бибилиотеки .
Для разведочных данных наиболее популярной является библиотека `pandas_profiling`
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import rcParams
import pandas_profiling
```
Установим параметры изображений
```python
%matplotlib inline
rcParams['figure.figsize'] = (16, 8)
```
Загрузим данные из файла whether_train.csv. Учтем что нам нужны данные только по одной локации site_id = 0
```python
data = pd.read_csv('wheter_train.csv')
data = data.loc[data['site_id']==0]
```
Выведем результат
```puthon
print(data.head())
```
выведем график температуры окружающего воздуха
```python
data.set_index('timestamp')['air_temperature'].plot()
plt.show()
```
Получим общие данные по всему DataFrame
```python
data.describe()
```
Другой способ получить общую иформацию - команда
```python
pandas_profiling.ProfileReport(data, explorative = True)
```
	Вообще под Разведочным Анализом данных (Exploratory Data Analysis- EDA) подразумевают предварительное исследование Датасета c целью определения его основных характеристик, взаимосвязей между признаками , а также сужения наборов методов используемых для Машинного Обучения

EDA разбивают на :
**Удаление дубликатов**
Прежде посмотрим исходный размер датасета
```python
print(data.shape)
```
Удалим дубликаты (если они есть) и посмотрим как изменится размер датасета
```python
data = data.drop_duplicates(data)
print(data.shape)
```
**Обработка пропусков**
Слудет помнить что если пропусков более 75% признак удаляют. Воспользуемся методом isnull
```python
data = data.isnull()*100
```


###### **Тема 6. Процесс ETL и очистка данных**
ETL это абревиатура Extract Transform Load (извлечение, преобразование, выгрузка)
Для решения задачи регресси будем использовать три csv файла:
* building_metadata.csv - данные о здании
* weather_train.csv - данные о погоде
* train.0.0.csv.gz - данные о потреблении
Прежде загрузим нудные библиотеки
```python
import pandas as pd
import numpy as np
```
Сформируем датасет для здания
```python
building = pd.read_csv('building_metadata.csv')
```
В датасете содержатся следующие данные:
* primary_use -использоваие
* square_feet - площадь здания
* year_build - год постройки
* floor_count - этажность
Теперь загрузим данные по погоде
```python
weather = pd.read_csv('weather_train.csv')
```
В датасете содержаться данные:
* air_temperature - температура воздуха $C^o$ 
* dew_temperature - точка росы $C^o$
* cloud_coverage - облачност 
* precip_deph_1_hr - осадки мм/час
* sea_level_pressure - давление мбар
* wind_speed - скорость ветра
* wind_direction - направление ветра( роза ветров)
Сформируем датасет для потребления
```python
energy_0 = pd.read_csv('train.0.0.csv.gz')
```

Теперь объединим два датасета сначала потребление и погоду . Соеденять будем по столбцу build_id
```python
energy_0 = energy_0.megre(left=energy_0, right=build, how = 'left', 
						 left_on = 'build_id', rigpht_on ='build_id')
```
Для объединения следующих датасетов прежде установим индексы для объединения
```python
energy_0.set_index('timestamp', 'site_id', inplace=True)
weather.set_index('timestamp', 'site_id', inplace=True)
```
Теперь объеденим два датасета по индексам. 
```python
energy_o = energy_0.megre(left=energy_0, right =weather, how='left',
						 left_index=True, right_index=True)
```
Теперь сбросим индексы
```python
energy_0.reset_index(inplace=True)
```
Теперь заполним пропущенные значения по следующим правилам:
air_temperature : NaN -> 0
cloud_coverage : NaN -> 0
dew_temperature : NaN -> 0
precip_depht_1_hr :NaN -> 0
sea_level_pressure : NaN - заполним сердним значением
wind_direction : NaN - также среднее значение

Сначала вычислим средние значения для давления и направления ветра
```python
energy_0_sea_level_pressure_mean = energy_0['sea_level_pressure'].mean()
energy_0_wind_direction_mean = energy_0['wind_direction'].mean()
```
теперь заплним пропущенные значения
```python
energy_0['air_temperature'].fillna(0, inplace = True)
energy_0['cloud_coverage'].fillna(0, inplace = True)
energy_0['dew_temperature'].fillna(0, inplace = True)
energy_0['precip_depht_1_hr'] = energy_0['precip_depht_1_hr'].apply(lambda x : x if x >0 else 0)
energy_0['sea_level_pressure'] = energy_0['sea_level_pressure'].apply(lambda x : energy_0_sea_level_pressure_mean if x != x else 0)
energy_0['wind_direction'] = energy_0['wind_direction'].apply(lambda x : energy_0_wind_direction_mean if x != x else 0)
```
ввывод результата
```python
energy_0.info()
```

###### **Тема 7. Обучающая, проверочная и валидационные выборки**
Для создания тестовой и обучающие выборок пройдем всю цепочку обработки данных из файлов building_metadata.csv, train.0.0.csv.gz, weather_train.csv.
Прежде загрузим необходимые бибилиотеки
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
```
Загрузим библиотеки
```python
building = pd.read_csv('building_metadata.csv')
weather = pd.read_csv('weather_train.csv')
energy_0 = pd.read_csv('train.0.0.0.csv')
```
Объеденим данные сначала сдлеаем это с данными energy_0 и building
```python
energy_0 = energy_0.merge(left = energy_0, right = build, how ='left',
						 left_on ='building_id', rigth_on = 'building_id')
```
Для объеденения полученного датасета с данными по погоде сначала установим индексы
```python
energy_0.set_index('timestamp', 'site_id', inplace = True)
weather.set_index('timestamp', 'site_id', inplace =True)
```
Произведем объеденение
```python
energy_0 = energy_0.merge(left = energy_0, rigth = weather, how = 'left',
						 left_index = True, rigth_index = True)
```
Cбросим индексы
```python
energy_0.reset_index(inplace =True)
```
Вычислим среднее значение для столбцов sea_level_pressure, wind_direction
```python
energy_0_sea_level_pressure_mean = energy_0['sea_level_pressure'].mean()
energy_0_wind_direction_mean = energy_0['wind_direction'].mean()
```
Заполним пропуски датасета
```python
energy_0['air_temperature'] = energy_0['air_temperature'].interpolate()
energy_0['cloud_coverage'].fillna(0, inplace = True)
energy_0['dew_temperature'].fillna(0, inplace = True)
energy_0['precip_depht_1_hr'] = energy_0['precip_depht_1_hr'].apply(lambda x:x if x>0 else 0)
energy_0['sea_level_pressure'] = energy_0['sea_level_pressure'].apply(lambda x:energy_0_sea_level_pressure_mean if x !=x else 0)
energy_0['wind_direction'] = energy_0['wind_direction'].apply(lambda x:energy_0_wind_direction_mean if x !=x else 0)
```
Сохраним промежуточный результат
```python
energy_0.to_csv('energy_0.csv')
```
Проведем обогащение данных
Прежде загрузим новый датасет
```python
data = pd.read_csv('energy_0.csv')
```
Заменим wind_direction на два столбца wind_direction_cos и wind_direction_sin
```python
data['wind_direction_rad'] = data['wind_direction']*np.pi/100
data['wind_direction_sin'] = np.sin(data['wind_direction_rad'])
data['wind_direction_cos'] = np.cos(data['wind_direction_rad'])
```
Сбросим столбец
```python
data = data.drop(lables=['wind_direction_rad'], axis =1)
```
Добавим столбец первой производной от air_temperature
```python
data['air_temperatuer1'] = data['air_temperature'].diff()
data.at[0, 'air_temperatuer1'] = data.at[1, 'air_temperatuer1']
```
Удалим не нужные столбцы
```python
data = data.droop(labels = ['site_id', 'building_id', 'meter', 'primary_use', 'square_feet', 'year_build', 'floor_count', 'wind_direction'], axis = 1)
```
Сохраним промежуток данных
```python
data.to_csv('energy_1.csv')
```
Разобъем данные на тестовую и проверочную выборки
```python
data_train, data_test = train_test_split(data, test_size = 0.2)
```
Создадим модель
```python
model = float(data_train['meter_reading'].mean())
```
Оибка модели на тестовой выборке
```python
err = np.mean(np.abs(data_test['meter_reading'] - model))
```
Вывод ошибки
```python
print('{0:.4}%'.format(100*err/model))
```
Домашнее задание:
Выделить час из timestamp
Вычислить среднее значение электропотребления по часам на обучающей выборке
Проверить точность модели "среднее по часам" относительно модели "общее среднее" на проверочной выборке


###### **Тема 8. Смещение, разброс и ошибка данных в модели**
 В моделях машинного обучения различают два вида неточностей ( ошибок):
 * Смещение - `bias`
 * Разброс - `variance`
 Смещение это когда модель выдает близкие по значению результаты, но все они имеют некоторое смещение относительно абсолютно точного результата . Следует учесть , что это смещение от желаемого результата может быть как маленьким ( говорят о `low bias`), так и достаточно большим (`high bias`), но все эти результаты имеют достаточно близкие друг к другу значения. Причем это неявляется ошибкой данных (нельзя просто взять и прибавить/убавить какое то значение к данным ) это именно ошибка самого алгоритма ( каждая модель имеет некоторое смещение обусловленное именно алгоритмом).
 Разброс это когда одна и та же модель при разных точках инициализации выдает резко отличающиеся друг от дргуа результаты. Т. е. на одних и тех же данных на разных циклах иттерации выдает совершенно разные результаты.
 Проблема смещения решается использованием `разных типов моделей` с различными смещениями результата( желательно чтобы смещения каким то образом компенсировались) и вычислением средней ошибки . 
 Разброс ошибок нивелирется использованием множества `однотипных` моделей и опорой на среднее значение ошибки
 Наиболее существенной ошибкой является `статистичекая ошибка данных` либо `статистическая ошибка самой модели`, которая проистекает из ошибки в самих данных.
 Т. е. если речь идет о энергопотреблении и при условии что наши данные дастаточно точны, но сами данные имеют статистическую ошибку в 0.1 (точность самого прибора к примеру не превышает 0.1), то ошибка в предсказаниях не может быть менее 0.1 (это ошибка не данных, не алгоритма обработки данных, но ошибка самих данных)
 Хорошо если окажется что разброс модели будет меньше чем разброс в самих данных
 Исходя из сказанного различают ансамбли (наборы) моделей 
 * Ансамбль Беггинга - набор однородных моделей
 * Ансамбль Стекинга - набор разнородных моделей
 **Практикум**
 Ошибка данных и моделей
 Прежеде загрузим библиотеки
 ```python
 import pandas as pd
 import numpy as np
 import datetime 
```
Из ранее обработанных данных выделим еще одну колонку ( время в часах)
```python
data = pd.read_csv('energy_1.csv')
```
Выделим время
```python
data['timestamp'] = pd.to_datetime(data['timestamp'])
data['hour'] =data['timestamp'].dt.hour
```
Сохраним промежуточный результат
```python
data.to_csv('energy_2.csv', index = False)
```
Ошибка данных
```python
std = pd.std(data['meter_reading'])
```
Вывод ошибки данных
```python
print('{0:.4}'.format(std))
```
Модель
```python
model = np.mean(data['meter_reading'])
```
Или
```python
model = float(data['meter_reading'].mean())
```
Ошибка модели
```python
err = float(np.abs(data['meter_reading']-model))
```
Вывод
```python
print('Модель: {0:.4}'.format(model),
	 'Ошибка данных : {0:.4}%'.format(100*std/model),
	 'Ошибка модели : {0:.4}%'.format(100*err/model))
```
засчитаем ошибку для часа 0
Выделим данные на период с 0 по 1
```python
data0 = data.loc(data['hour']==0)
```
Модель
```python
model0 = np.mean(data0['meter_reading'])
```
Ошибка модели по часу
```python
err0 = np.mean(np.abs(data0['meter_reading']-model0))
```
Общий вывод
```python
print('Модель : {0:.5}'.format(model0),
	 'Ошибка модели по часу : {0:.4}%'.format(100*err0/model0),
	 'Ошибка на базовой модели : {0:.4}%'.fromat(100*err/model))
```

###### **Тема 9. Недообучение и переобучение модели**
Различают три состояния моделей:
* модель недоучилась
* модель обучилась
* модель переобучилась
Переобученная модель на новых (неизвестных) данных выдает ошибку большую чем обученная модель. Обученная модель выдает ошибку приблизительно одинаковую для новых (неизвестных) данных так и для уже известных модели данных ( обучающая, тестовая, валидационная выборки). 
Таким образом тот факт что при обучении модель выдает точность 100% скорее всего говорит о переобучении модели. Такая логика работает на моделях машинного обучения - в нейронных сетях дело обстоит сложнее.
Практикум:
Загрузим нужные нам библиотеки
```python
import pandas as pd
import numpy as np
import glob
import datetime
```
Посмотрим на файлы с расширением csv
```python
files_list = glob.glob('*.csv')
```
Загрузим сохраненные данные прошлых заняятий
```python
data = pd.read_csv('energy_2.csv')
```
Построим модель
```python
model = float(data['meter_reading'].mean())
```
Ошибка модели
```python
err = np.mean(np.abs(data['meter_reading'] - model))
```
Вывод результата
```python
print('Модель : {0:.5}'.format(model),
	 'Ошибка на модели: {0:.4}%'.format(100*err/model))
```
Теперь выделим из данных столбец с указанием дня замера
```python
data['timestamp'] = pd.to_datetime(data['timestamp'])
```
собственно сам столбец
```python
data['day'] = data['timestamp'].dt.day
```
Сохраним промежуточный результат
```python
data.to_csv('energy_3.csv', index = False)
```
Отбрем данные касаемого одного дня (31 числа)
```python
data31 = data.loc[data['day']==31]
```
Построим модель
```python
model31 = float(data31['meter_reading'].mean())
```
Расчитаем ошибку модели:
```python
err31 = np.mean(np.abs(data31['meter_reading'] - model31))
```
Вывод результатов расчетов:
```python
print('Модель : {0:.5}'.format(model31),
	 'Ошибка на базовой модели : {0:.4}%'.format(100*err/model),
	 'Ошибка на модели по дню : {0:.4}%'.format(100*err31/model31))
```

**Задание**
1. Вычислить ошибки по дням от 1 до 31
2.  Вычислить ошибку ансамбля : среднюю ошибку всех моделей по дню. Сравнить ее с ошибкой ансамбля моделей по часам






###### **Тема 10. Использование HDF**
Для загрузки данных используются файлы различных форматов . 
Такие как :
* Файлы с расширением CSV и TSV - текстовые файлы с разделителем
* Файлы JSON файлы имеющие иерархическуюс труктуру
* Файлы MesagePack - этот и все последующие форматы уже работатю со столбцами данных
* Файлы HDF5 - используют для сохранения моделей и их весовых коэффициентов
* Файлы Feather
* Файлы Parquet
Форматы MesagePack , HDF5(возможно), Feather, Parquet используют для лучшего использования оперативной памяти и вычислительных ресурсов собственные словари для лучшей компрессии данных
**Практика**
Сравнительный анализ размеров файлов разных форматов.
Для практики используется файл `energy_2.csv`
Загрузим библиотеки
```python
import os
import pandas as pd
import numpy as np
import msgpack
import h5py
import pyarrow.feather as feather
```
Для файлов `csv`
Загрузка
```python
data = pd.read_csv('energy_2.csv')
```
Вывод размера файла
```python
print('Размер CSV файла :', round(os.stat('energy_2.csv').st_size/1024), 'kb')
```
Теперь преобразуем тип данных в массиве data
```python
data['hour'] = data['hour'].astype(np.uint8)
```
Для остальных столбцов приведем данные к типу float32
```python
for column in data.columns:
	if column not in ['hour', 'timestamp']:
		data[column] = data[column].astype(np.float32)
```
Теперь преобразуем в формат msgpack
```python
with open('energy_2.mpack', 'wb') as f:
	f.write(msgpack.packb(data.to_json(), use_bin_type = True))
```
Выведем размер mpack
```python
print('Размер mpack файла : ', round(os.stat('energy_2.mpack').st_size/1024), 'kb')
```
Для файлов HDF5
Преоброзование
```python
data.to_hdf('energy_2.h5','data', mode = 'w')
```
Вывод размера файла HDF5
```python
print('Размер файла HDF5 : ', round(os.stat('energy_2.h5').st_size/1024), 'kb')
```
Преобразуем данные файла data в feather
```python
feather.write_feather(data, 'energy_2.feather')
```
Выведем размер файла
```python
print('Размер файла Feather : ', round(os.stat('energy_2.feather').st_size/1024), 'kb')
```
Для Parquet преобразуем файл
```python
data.to_parquet('energy_2.parquet')
```
Результирующий размер
```python
print('Размер файла Parquet без компрессии : ', round(os.stat('energy_2.parquet').st.size/1024), 'kb')
```
Возможно преобразование со сжатием
```python
data.to_parquet('energy_2.parquet_br', compression = 'brotli')
```
Размер parquet файла с компрессией
```python
print('Размер parquet файла с компрессией : ', round(os.stat('energy_parquet_br').st_size/1024), 'kb')
```
###### **Тема 11. Метрики регрессионных моделей**
Используют следующие методики расчета метрик:
* расстояние Эвклида $\sqrt[2]{(x-y)^2}$ используются когда необходимо предсказание численных кортежей
* $max|x_i - y_i|$ - расстояние Чебышева используются когда необходимо предсказание численных кортежей при том что количество параметров стремится к бесконечности
* засстояние городских кварталов или расстояния Манхеттена используются когда необходимо предсказание категориальных значений кортежей
* $\sqrt[r]{\sum(x_i - y_i)^p}$ - растояние Минковского является обощенной формулой рассчета метрик

**Метрики задач регрессии**
Используют следующие метрики рассчета ошибок в задачах регрессии:
*Mean Squared Error*
$$MSE =\frac{1}{n}\displaystyle\sum_{i=1}^{n}{c_i^2}$$
*Root Mean Squared Error*
$$RMSE = \sqrt{\frac{1}{n}\displaystyle\sum_{i=1}^{n}{c_i}^2}$$
*Mean Absolute Error*
$$MAE = \frac{1}{n}\displaystyle\sum_{i=1}^{n}|{c_i}|$$
*Mean Absolute Perentage Error*
$$MAPE = \frac{100}{n}\displaystyle\sum_{i=1}^{n}{|\frac{c_i}{y_i}|}$$
Для исключения выбросов ошибок при средней квадратичной ошибки используют логарифмы:
*Root Mean Squared Error*
$$RMSE = \sqrt{\frac{1}{n}\displaystyle\sum_{i=1}^{n}(x_i - y_i)^2}$$
*Root Mean Squared Log Error*
$$RMSLE = \sqrt{\frac{1}{n}\displaystyle\sum_{i=1}^{n}(\log({p_i+1})-\log({a_i +1}))^2}$$
где $\log({p_i +1})$ - предсказание, а $\log({a_i + 1})$ - значение эталонное 

**Практикум по метрикам задач регрессии**

Вычислить среднее потребление электроэнергии по часам, построить ошибки моделей по метрикам MAE, MAPE, MSE, RMSE, RMSLE
Загрузим библиотеки
```python
import pandas as pd
import numpy as np
import glob
```
Посмотрим на файлы
```python
files_list = glob.glob('*.csv')
```
Загрузим данные
```python
data = pd.read_csv('energy_2.csv')
```
Выведем 10 первых строк
```python
print(data.head())
```
Создадим пустой список значений затем пробежим по значением и заполним список , затем выведе его 
```python
esemble = []
for h in range(24):
	esemble.append(data[data['hour']==h]['meter_reading'].mean())
print(esemble)
```
Теперь рассчитаем ошибки моделей
MAE
```python
err_mae = []
for h in range(24):
	err_mae.extend((data.loc[data['hour']==h]['meter_reading']-esemble[h]).values)
print('Ошибка MAE составляет: {0:.4}%'.format(np.abs(err_mae).mean()))
```





















###### **Тема 12. Метод наименьших квадратов**
До сих пор анализ данных и попытки выдать прогноз проводился без увязки с зависимостями. Если предположить что энерго потребление напрямую зависит от внешней температуры , то задача регрессии сводятся построению зависимости потербления от температуры.
Простешими методами являются метод макисмального правдоподобия и метод наименьших квадратов.
Если предположить что энергопотребление y (meter_reading) линейно зависит от x (air_temperature), тогда решение задачи регрессии сводится кнахождению коэфициентов а и b таких что
$$ y = a*x + b $$
Для нахождения коэффициентов будем использовать данные energy_2.csv
Прежде загрузим библиотеки и данные в переменную data
```python
import pandas as pd
import numpy as np
# Загрузим данные из файла
data = pd.read_csv('energy_2.csv')
```
Так как будет использован метод наименьших квадратов то необходимо найти
```python
n = len(data)
sumx = np.sum(data['air_temperature'])
sumy = np.sum(data['meter_reading'])
sumx2 = np.sum(data['air_temperature']*data['air_temperature'])
sumxy = np.sum(data['air_temperature']*data['meter_reading'])
#Коэффицент a высчитывается
a = (n*sumxy - sumx*sumy)/(n*sumx2 - sumx**2)
# Коэффициент b рассчитывается 
b = (sumy - a*sumx)/n
# Итоговый вывод
print('y =', a, '* x +', b)
```
Теперь расчитаем ошибку RMSLE для линейной зависимости
```python
err_rmsle = (np.log(1 + data['meter_reading'] - np.log(1 + a*data['air_temperature'] + b))**2
# Вывод ошибки
print('Ошибка RMSLE для линейной модели равна: {0:.5}'.format(err_rmsle.mean()**0.5))
```

###### **Тема 13. Модель линейной регрессии**
Регрессия и корреляция
$$r_{\frac{y}{x}}=\frac{\displaystyle\sum_{i=1}^{n}(x_i -\bar x)*(y_i -\bar y)}{\sqrt{\displaystyle\sum_{i=1}^{n}(x_i -\bar x)^2*(y_i -\bar y)^2}}$$
Нормализация параметров по методу минимакса
$$X_norm = \frac{X-X_{min}}{X_{max}-X_{min}}$$
Z-Нормализация
$$Z=\frac{x-\mu}{\sigma}$$
Условия использования линейной регресси:
* Нормальное распределение ошибок
Если максимальные значения ошибок сосредоточены возле нуля, то можно утвержадть что использование модели линейной регрессии оправдано
* Гомоскедастичность данных
Если данные равномерно распределены от некоторой прямой или криовой линии ( в отличии от гетероскедастичных данных, которые в разных частях прямой или кривой линии удалены на некоторое различное расстояние)
* Независимость праметров
Параметры не должны коррелировать между собой ( например параметр точки росы и параметр температуры окружающей среды очень сильно коррелируют между собой поэтому не могут использоваться в модели линейной регрессии)
**Практикум**
Загрузим библиотеки
```python
import pandas as pd
import numpy as np
from sklear.preprossesing import MinMaxScalear
from sklear.liner_model import Linear_Regression
```
Получим данные
```python
data = np.read_csv('energy_2.csv')
```
Нормализуем данные по методу минимакса
```python
data_norm = MinMaxScaler().fit_transform(data['air_temperature, sea_level_pressure'])
```
Модель линейной регрессии представляет собой
`model_reading = a*air_temperature + b*sea_level_pressure +c`
Сформируем модель
```python
x = data_norm
y = data['meter_reading']
model = LinerRegression().fit(x, y)
```
Вывод результата
```python
print('meter_reading =', int(model.coef_[0]), '+ air_temperature -', 
	 int(model.coef_[1]*(-1)), '+ sea_level_pressure -', int(medel.intercept_))
```
Коэффициент определенности
```python
print('Коэффициент определенности (R2):', model.score(x, y))
```
Расчет RMSLE
```python
err_rmsle = (np.log(1 + data['meter_reading'])-np.log(1 + model.predict(data_norm)))**2
```
Вывод
```python
print('Ошибка RMSLE : {0:.5}'.format(err_rmsle.mean()**0.5))
```

###### **Тема 14. Линейная регрессия с регуляризацией**
Ргуляризация - общий принцип оптимизации модели регресси, позволяющий выбрать такие гипрепараметры модели и сами параметры таким образом, чтобы обеспечить наиболее точную модель регрессии.
Различают :
`L1 регуляризация :`
$$L(W) = \frac{1}{N}\displaystyle\sum_{i=1}^{N}(f(x_{i},W).y_{i})+\lambda*R(W)$$

где
$$\frac{1}{N}\displaystyle\sum_{i=1}^{N}(f(x_{i},W).y_{i}$$
Потери данных:
прогнозы модели должны работать в обучающей выборке
а
$$\lambda*R(W)$$
Ругуляризация:
Модель должна работать на тестовой выборке
L1 регуляризация мимнимизирует сумму весовых коэфициентов. Т.е те параметры которые не влияют или мало влияют на результат предикта модели имеют такой коэффициент регуляризации $\lambda$ , который либо выводит сумму весовых коэффициентов в ноль , либо делает их очень малым ( близким к нулю). Таким образом те параметры которые не коррелируются с параметрами , котрые мы предсказываем имеют те те весовые коэффициенты, которые нивелируют влияние параметров на предикт модели.
`L2 регуляризация :`
 Минимизирует сумму КВАДРАТОВ весовых коэффициентов.

`ElasticNet`
 Комбинированный способ регуляризации, который собственно говоря есть сумма L1, L2 регуляризаций.
**Практикум**
Линейная регрессия с регуляризацией:
Загрузим библиотеки
```python
import pandas as pd
import numpy as np
from sklearn.perprossesing import MinMaxScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
```
Получим данные из energy_2.csv
```python
data = pd.read_csv('energy_2.csv')
```
Выяснилось что данные столбца wind_speed содержат значения NaN
Заполним 
Снасала высчитаем среднее значение wind_speed
```python
wind_speed_mean = data['wind_speed'].mean()
```
Потом lambda пройдемся
```python
data['wind_speed'] = data['wind_speed'].apply(lambda x: wind_speed_mean() if x != x else 0)
```
Нормализуем данные
```python
data_norm = MinMaxScaler().fit_transform(data[['air_temperature',
                                         'sea_level_pressure',
                                         'cloud_coverage',
                                         'dew_temperature',
                                         'precip_depth_1_hr',
                                         'wind_speed',
                                         'wind_direction_cos',
                                         'wind_direction_sin',
                                         'air_temperature1']])
```
Теперь создадим модель линейной регресси без регуляризации
```python
x = data_norm
y = data['meter_reading']
model = LinearRegression().fit(x, y)
```
Выведем коэффициенты модели и коэффициент R2
```python
print('Коэффициенты модели :', model.coef_, model.inercept_)
print('Коэффициент определенности R2:', model.score(x, y))
```
Рассчитаем RMSLE и выведем среднеквадратичное отклонение RMSLE
```python
err_rmsle = (np.log(1 + data['meter_reading'])-np.log(1 + model.predict(data_norm))**2

print('Среднеквадратичное отклонение RMSLE :{0:.5}'.format((err_rmsle).mean())**0.5))
```
Cформируем модель линейной регрессии с L1 регуляризацией
```python
model_l1 = Lasso(alpha = 0.1).fit(x, y)
print('Коэффициенты модели линейной регрессии с L1 регуляризацией:',
	 model_l1.coef_, model_l1.intercept_)
```
Коэффицент определенности для модели линейной регрессии с L1 регуляризацией
```python
print('Коэфицент определенности для линейной модели R2:', model_l1.score(x, y))
```
Рассчет ошибки RMSLE и вывод среднеквадратичного отклонения
```python
err_rmsle_l1 = (np.log(1 + data['meter_reading'])-np.log(1 + model_l1.predict(data_norm)))**2
print('Среденеквадратичное отклонение R2:{0:.5}'.format((err_rmsle_l1.mean())**0.5))
```
Теперь модель с L2 регуляризацией
```python
model_l2 = Ridge(alpha=10).fit(x, y)
print('Коэффициенты модели линейной регрессии при L2 регуляризации:',
	 model_l2.coef_, model_l2.intercept_)
```
Коэффициент определенности R2 
```python
print('Коэффициент определенности R2 для модели линейной регрессии с L2 регуляризацией:', model_l2.score(x, y))
```
Рассчитаем RMSLE ошибку и выведем среднеквадратичное отклонение
```python
err_emsle_l2 = (np.log(1 + data['meter_reading'])-np.log(1 + model_l2.predict(data_norm)))**2
print('Среднеквадратичное отклонение RMSLE: {0:.5}'.format((err_rmsle_l2.mean())**0.5))
```

###### **Тема 15. Оптимизация гиперпараметров модели**
Оптимизация гиперпарметров модели 
$$L(W) = \frac{1}{N}\displaystyle\sum_{i=1}^{N}L_i(f_iW) -\lambda*R(W)$$
Сводится к нахождению оптимального $\lambda$ 
Воможно рассматривать последовательную зависмость $\lambda$ от ОДНОГО параметра, тогда используют либо `равномерное` распределение $\lambda$  в пределах какогото диапазона - GRID SEARCH. Возможно рассматривать `случаное` распределение $\lambda$  в пределах дапазона - RANDOM SEARCH  . Если же рассматривать зависимость $\lambda$  от  нескольких парметров то пространство параметров случайным образом заполняется значениями $\lambda$  таким образом чтобы закрыть все это пространство. Восзможно рассматривать зависимость $\lambda$ по расспеределению ГАусса - scikit-optimize. Для зависимости $\lambda$  от нескольких параметров используют Latin Hypercube
**Практикум**
Для практикума используем energy_2.csv
Загрузим библиотеки
```python
import optuna
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.preprosessing import MinMaxScaler
from sklear.model_selection import RandomSearchCV
from skearn.stats import uniform
from skearn.metrics import mean_squared_log_error
```
Получим данные
```python
data = pd.read_csv('energy_2.csv')
```
Заполним пропуски в столбце wind_speed средним значением
```python
wind_speed_mean = data['wind_speed'].mean()
data['wind_speed'].apply(lambda x:x if x != x else x == x)
```
Нормализуем данные
```python
data_norm = MinMaxScaler().fit_transform(data[['air_temperature',
                                         'sea_level_pressure',
                                         'cloud_coverage',
                                         'dew_temperature',
                                         'precip_depth_1_hr',
                                         'wind_speed',
                                         'wind_direction_cos',
                                         'wind_direction_sin',
                                         'air_temperature1']])
```
Cоздадим функцию обработки ошибок
```python
def rmsle_err(y, y_pred):
	return ((np.log(1 + y)-np.log(1 + y_pred))**2).mean()**0.5
```
Создадим модель без зегуляризации
```python
x = data_norm
y = data['meter_reading']
model = LinearRegression().fit(x, y)
print('RMSLE :{0:.5}'.format(rmsle_err(y, model.predict(x))))
```
Случаная оптимизация - поиск оптимальной пары значений alpha и l1ratio 
Модель
```python
model_el = ElasticNet()
```
Создадим словарь пар alpha и l1ratio (случайной выборкой)
```python
distributions = dict(alpha = uniform(loc = 0, scale =1),
				   l1_ratio = uniform(loc = 0, scale =1))
```
Из словаря выборка случайных пар значений
```python
clf = RamdomizeSearchCV(model_el, distributions, random_state=0)
```
Обучим clf
```python
search = clf.fit(x, y)
```
Выведем
```python
print(search.best_params_)
```
полученные параметры используем в модели
```python
model_el = ElasticNet(alpha = search.beast_params_['alpha'],
					 l1_ratio = search.best_params_['l1_ratio'])
print('RMSLE :{0:.5}'.format(rmsle_err(y, model_el.predict(x))))
```
###### **Тема 16. Изотоническая регрессия**
Линейная регрессия учитывая общий тренд выстраивает линию между начальной и конечной точками(потому она и называется линейной).
В реалиях разброс точек от этой кривой может быть весьма велик. И неиболее точно соединяет апрксимирует) эти точки политоническая регрессия. Но просторить кривую n-ого порядка сразу сложно. Промежуточным этапом между линейной и политоничекой регрессиями является изотоническая регрессия 
(Почти) Изтотоническая регрессия
$$minimize_{\beta}\;\frac{1}{2}\displaystyle\sum_{i=1}^{m}(y_i -\beta_i)^2+\lambda\displaystyle\sum_{i=1}^{m-1}(\beta_i -\beta_{i-1})_+$$
Апроксиматическая линия линейной регрессии делиться на некоторые участки - таким образом получается некоторая ступенчатая кривая, которая имеет некоторое приближение к политонической регрессии. Такая кривая называется Кусочно_Линейной регрессией. Кусочно_линейная регрессия НЕ учитывает отрицательные тренды (они рассматриваются как некий шум). Также используют помимо почти изотонической регрессии полная изотоническая, где отрицательные тренды учитываются тем самым оставаясь изотонической кривой приближается к политонической регрессии
**Практикум**
Для практикума используем данные из energy_2.csv
загрузим библиотеки
```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.isotonic import IsotonicRegression
```
Загрузим данные
```python
data = pd.read_csv('energy_2.csv')
```
В силу того что будет использован всего один параметр 'air_temperature', нормализация данных не потребуется
Зададим функцию обработки ошибок
```python
def rmsle_err(y, y_pred):
	return ((np.log(1 + y)-np.log(1 + y_pred))**2).mean()**0.5
```
Сформируем модель линейной регрессии без регуляризации
```python
x = np.array(data['air_temperature']).reshape(-1, 1)
y = data['meter_reading']
```
Сама модель линейной регрессии
```python
model = LinearRegression().fit(x, y)
	print('RMSLE:{0:.5}'.format(rmsle_err(y, model.predict(x))))
```
Изотоническая модель будет 
```python
model_is = IsotonicRegression().fit(x, y)
	print('RMSLE :{0:.5}'.format(rmsle_err(y - model_is(x))))
```

###### **Тема 17. Полиномиальная регрессия**
Полиномиальная регрессия это способ сформировать некий полином , который максимально приближен к набору входных данных. Это достигается нахождением некоторого степенного ряда. 
Для определения оптимального значения степени полиномиального ряда используют `информационные критерии`:
$$AIC\;=\ln{(\frac{1}{n}\sum e^2)}+\frac{2k}{n}$$
$$BIC\;=\ln{(\frac{1}{n}\sum e^2)}+\frac{k}{n}\ln{(n)}$$
где 
$\ln{(\frac{1}{n}\sum e^2)}$  - это логарифм среднеквадратичной ошибки модели 
n - число входящих данных
k - коэффициент сложности (степень) число данных принятых к учету
**Практика**
Для практики используем набор данных energy_2.csv
Загрузим библиотеки
```python
import pandas as pd
import numpy as np
import optuna
from sklearn.linear_model import LinearRegression, ElasticNet
from sklearn.preprosessong import MinMaxScaler
from sklearn.metrics import mean_squared_log_error
```
Загрузим данные
```python
data = pd.read_csv('energy_2.csv')
```
Заполним пропуски в столбце wind_speed
```python
data['wind_speed'].fillna(0, inplace = True)
```
Получим данные второго порядка т.е. мы 10 столбцов данных перемножим друг с другом. В модель таким образом будет загружено не 10 праметров а 100 + 10 = 110 праметров.
Расчитаем эти парметры и исключим из параметров столбец timestamp (временная отметка на сами результаты не влияет) и столбец meter_reading (его мы предсказываем). Таким образом сформируем данные
```python
columns_iterate = data.columns
columns = list(data.columns)
for column1 in columns_iterate:
	for column2 in columns:
		if (column1 not in ['timestamp', 'meter_reading'] and
			column2 not in ['timestamp', 'meter_reading']):
			c = column1 +'_'+column2
			data[c] = np.multiply(data[column1], data[column2])
			columns.append(c)
columns.remove('timestamp')
columns.remove('meter_reading')
```
Теперь нормализуем данные
```python
data_norm = MinMaxScaler().fit_transform(data[columns])
```
Сформируем данные и создадим модель
```python
x = data_norm
y = data['meter_reading']
model = LinearRegression().fit(x, y)
```
Функция расчета ошибки
```python
def rmsle_err(y, y_pred):
	return ((np.log(1 +y)-np.log(1 + y_pred))**2).mean()**0.5
```
вывод
```python
print('RMSLE ошибка : {0:.5}'.format(rmsle_err(y, model.predict(x))))
```
Теперь содадим модель optuna
```python
def objective(trial):
	alpha = trial.suggest_float('alpha', 1e-8, 1, log = True)
	l1_ratio = trial.suggest_float('l1_ratio', 1e-3, 1, log = True)
	regressor_obj =ElasticNet(alpha = alpha,
							l1_ratio = l1_ratio, max_iter = 10000)
	regressor_obj.fit(x, y)
	y = regression_obj.predict(x)
	return mean_squared_log_error
```
Вызовим фукцию и обучим модель
```python
study = optuna.create_study()
study(objective, n_trial = 1000)
```
Вывод результата
```python
print(study.best_params)
```
Исходя из самых оптимальных результатов построим модель
```python
model_optuna = ElasticNet(alpha = study.best_params['alpha'],
						 l1_ratio = study.best_params['l1ratio'],
						 max_iter = 10000000)
```
вывод 
```python
print('RMSLE ошибка модели optuna :{0:.5}'.format(rmsle_err(y, model_optuna(x))))

```

###### **Тема 18. Линеаризация регрессии**
Линеаризация регрессии - это такой подход при котором за счет преоразований переменных нелинейная зависимость праметров приводится к линейной зависисмости
Пример таких преобразований :
| Функция       | Исходное уравнение  | Преобразованное уравнение                               |
| ------------- | ------------------- | ------------------------------------------------------- |
| Гипербола     | $y =a +\frac{b}{x}$ | $\frac{1}{x}=z\;\;\;y=a +bz$ |
| Степенная     | $y = ax^b$          | $\ln(y)=\ln(a)+b\ln(x)$                                 |
| Показательная | $y=ab^x$            | $\ln(y)=\ln(a)+x\ln(b)$                                 |
| Экспонента    | $y=e^{a+bx}$        | $\ln(y)= a+bx$                                          |
**Практика**
Будет использован набор данных energy_0.csv
Загрузим библиотеки:
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%% matplotlib.pyplot inline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
```
Получм  данные
```python
data = pd.read_csv('energy_0.csv')
print(data.head())
```
подготовим данные
```python
x = np.array(data.index).reshape(-1, 1)
y = data['air_temperature']
```
На основе этих данных посчитаем коэффициентопределенности R2. А также расчитаем коэффициенты BIC и AIC
**Модель линейной регрессии**
Функции рассчета AIC , BIC
Напомним что коэффициенты расчитываются по формулам
BIC - $BIC=\ln(\frac{1}{n}\sum{e^2}) + \frac{k}{n}\ln(n)$ 
AIC - расчитывают по формуле $AIC = \ln(\frac{1}{n}\sum{e^2}) + 2\frac{k}{n}$ 
Обратим внимание на то что $\ln(\frac{1}{n}\sum{e^2})$  Это логарифм средней квадратической ошибки . И BIC и AIC в этой части не отличаются . Их отличия в коэффициенте (его еще называют штрафом) Для BIC это $\frac{1}{n}\ln(n)$ , для AIC $2\frac{k}{n}$
Исходя из этого и учитыва что $n =len(y)$
выведем функцию рассчета BIC
```python
def calculate_bic(y, y_pred, power):
	return len(y)*np.log(len(y)*mean_squared_error(y, y_pred)**2) + power*np.log(len(y))
```
Функция AIC
```python
def calculate_aic(y, y_pred, power):
	return len(y)*np.log(len(y)*mean_squared_error(y, y_pred)**2) + 2*power
```
Зададим пустой список BIC и AIC, сформируем линейную регрессию, выведем R2 и заполним (а так же выведем) начальные значения AIC и BIC
```python
bics = [0]
aics = [0]
model = LinearRegression().fit(x, y)
bics.append(calculate_bic(y, model.predict(x), 1))
aics.append(calculate_aic(y, model.predict(x), 1))
print('R2 линейной регрессси :{0:.5}'.format(model.score(x,y)))
print('BIC линейной регрессси :{0:.5}'.format(bics[1]))
print('AIC линейной регресси :{0:.5}'.format(aics[1]))
```
**Степенная регрессия**
Создадим словари датафреймов для различных степеней
```pytnon
x_=pd.DataFrame({'x1': data.index})
```
Для 10 степеней сформируем модели и высчитаем - выведем R2, BIC, AIC
```python
for i in range(10):
	x_['x'+str(i+2)]=np.multiply(x_['x'+str(i+1)], data.index)
	model = LinearRegression().fit(x_, y)
	aics.append(calculate_aic(y, model.predict(x_), i+2))
	bics.append(calculate_bic(y, model.predict(x_), i+2))
	print('Регрессия',str(1+2), 'степени имеет R2 :{0:.5}'.format(model.score(x_)),
	'BIC :{0:.5}'.format(bics[i+2],
	'AIC :{0:.5}'.format(aics[i+2]))
	
```
Построим график полученных AIC и BIC
```python
aics[0] = max(aics)
bics[0] = max(bics)
plt.plot(aics)
plt.plot(bics)
plt.legend(['AIC', 'BIC'])
plt.show()
```
**Приближение по экспоненте**
```python
t = np.log(1 + y)
z = np.log(1 + x)
model_e = LinearRegression().fit(z, t)
print('R2 экспотеницальной модели :{0:.5}'.format(model_e.score(z, t)))
print('BIC :{0:.5}'.format(calculate_bic(t, model_e.predict(t), 1)))
print('AIC :{0:.5}'.format(calculate_aic(t, model_e.predict(t), 1)))
```
Для степенной экспотенциальной регрессси
```python
t = pd.DataFrame(t)
t_columns = ['t1']
for i in range(10):
	t_['t'+str(i+2)]=np.multiply(t_['t'+str(1+1), t[0]])
	model_r_ = LinearRegression().fit(t, y)
	print('R2 коэффициент экспотенциальной регресси степени', i+2, model_r_.score(t, y))
```
###### **Тема 19. Ансамбли моделей машинного обучения**
Различаюи сильные и слабые модели. Сильной считается модель, которая на выходе выдает    
приблизительно одинаковые результаты ( имеет малый разброс ), но всегда ошибается ( имеет смещение). Слабая модель имеет разброс выходных параметров и не имеет явно выраженного смещения результатов. Если слабая модель все же попадает в матожидание , такую модель можно использовать в ансамблях моделей.
Рзаличают несколько типов ансамблей :
* Ансамбль Беггинга используют СЛАБЫЕ модели которые "ставят" рядом т.е модели не влияют друг на друга и выдают автономные результаты. Полученые результаты ансамбля беггинга усредняются
* Ансамбль бустинга  - использует слабые модели результат работы которых влияет на остальные модели. Модели как бы стоят друг над другом 
* Скетинг моделей уже использует сильные модели . Ансамбль беггинга или ансамль бустинга считается сильными моделями. Результатом скетинга моделей является общий результат, который определяется методом голосования 
* Блендинг или мета ансамбль - это когда считается что ансамбли моделей (беггинга, бусинга. сктинга) принимается за слабую модель на основе этого формируется набор данных и происходит дальнейшее обучение ансамблей беггинга, бустинга, скетинга
Скетинг моделей ограничен в силу того что сами модели имеют конечное количество типов моделей и их архитектур
**Практикум**
Построить ансамбль линейной и изотонической регрессии. Для этого используем набор данных energy_2.csv. Построим 2 модели энергопотребления - линейную модель зависимости от давления, температуры, превой производной температуры, изотоническую модель  зависимости от температуры.
Загрузим библиотеки
```python
import optuna
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.isotonics import IsotonicRegression
from sklearn.metrics import mean_squared_log_error
from sklearn.preprosessing import MinMaxScaler
from sklearn.model_selection import train_test_split
```
Загрузим данные
```python
data = pd.read_csv('energy_2.csv')
```
Нормализуем данные 
```python
data_norm = pd.DataFrame(MinMaxScaler().fit_transform(data[['air_temperature', 'sea_level_pressura', 'air_temperature1']]))
```
делим данные на обучающую и тестовые выборки
```python
train, test, y_train, y_test = train_test_split(data_norm, data['meter_reading'], test_size = 0.2)
```
объявим функцию рассчета RMSLE ошибки
```python
def rmsle_err(y, y_pred):
	return ((np.log(1 + y)-np.log(1 + y_pred))**2).mean()**0.5
```
Построим модель линейной регрессии
```python
y = data['meter_reading']
model1 = LinearRegression().fit(train, y_train)
print('RMSLE ошибка линейной регрессии :{0:.5}'.format(rmsle_err(y_train, model1.predict(train))))
```
Изотоническая регрессия
```python
model2 = IsotonicRegression(out_of_bounds ='clip').fit(train[0], y_train)
print('RMSLE ошибка изотоноической регрессии :{0:.5}'.format(rmsle_err(y_train, model2.predict(y_train[0])))
```
Объединение моделей
Используем optuna для поиска наиболее оптимального коэффициента
```python
def objective(trial):
	alpha = trial.suggest_float('alpha', 1e-10, 1, log = True)
	y_pred = (alpha*model1.predict(test)+(1-alpha)*model2.predict(test[0]))
	return mean_squared_log_error
```
запустим обучение
```python
study = optuna.create_study()
study.optimize(objective, n_trial = 100)
```
расчитаем средневзвешенное значение
```python
y_pred1 = model1.predict(data_norm)
y_pred2 = model2.predict(data_norm[0])
y_pred = (study.best_params['alpha']*y_pred1 + (1-study.best_params['alpha'])*y_pred2)
```
вывод результата
```python
print('RMSLE ошибка линейной регрессии : {0:.5}'.format(rmsle_err(y, y_pred1)))
print('RMSLE ошибка изотонической регрессии :{0:.5}'.format(rmsle_err(y, y_pred2))
print('RMSLE ошибка ансамбля :{0:.5}'.format(rmsle_err(y, y_pred)))
```
###### **Тема 20. Ансамбль стекинга**

Ансамбль стекинга строится на сильных моделях. Сильные модели в ансамбле стекинга могут объеденятся различными способами:
* Мажоританое голование :
	предположим сущесвуют n моделелей model, каждая из которых выдает результат $model_n$ .predict = $pred_n$ . Тогда итоговая модель строится на модели с max ( $pred_n$ )  
* Усредненнное голосование:
	пусть существуют n моделей model , каждая из которых выдает результат $model_n$ .predict = $pred_n$ . Среднее $k =\frac{1}{n}\displaystyle\sum_{i=1}^{n}(pred_n)$. Тогда итоговая модель строится на модели k, которая находится $pred_m$ < k < $pred_{m+1}$
* Взвешенное голосование :
	Это когда каждому предикту модели присваивается некий весовых коэффициент . Например велеличина энергопотребления зависит от праметров температуры окружающегое воздуха , но она также зависит от дня недели ( возможно что энергопотребелние жилого здания возрастает в выходные дни - в рабоиче дни возрастает потребление офисных зданий). Но зависимость от температуры у энергопотребления выше чем  от дня недели (зачение предикта от температуры имеет более высокий весовой коэффициент).

**Практика**
Сравнение ансамблей стекинга . 
Ансамбль стекинга.
Найдем лучший степенной коэффициент, используя модель линейной регрессии. Построим на факторах линейной регрессии модель линейной регрессии, ElasticNet (с регуляризацией гиперпарметров) и изотоноическую модель. Посторим ансамбль стекинга.
Будет использованы данные energy_2.csv
Загрузим библиотеки
```python
import optuna
import pandas as pd
import numpy as np
from skelarn.linear_model import LinearRegression
from sklearn.isotonics import IsotonicRegression
from skelarn.preprosessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_squared_log_error
from skearn.model_selection import tarin_test_split
```
Загрузим данные 
```python
data = pd.read_csv('energy_2.csv')
```
Теперь последовательно переберем все линейные , квадратичные, кубические параметры из исходных , чтобы найти используя BIC
Функция рассчёта BIC
```python
def calculate_bic(y, y_pred, power):
	return len(y)*np.log(len(y)*mean_squared_error(y, y_pred)**2) + power*np.log(len(y))
```
Удалим столбцы, которые не учавствуют в предиктах
```python
columns = list(data.columns)
columns.remove('timestamp')
columns.remove('hour')
columns/remove('meter_reading')
```

**`Линейная регрессия`**
Начальные значения списка лучших столбцов, текущих столбцов
```python
best_columns =[]
current_columns = []
```
Начальные значения 
```python
best_power = 0
bic_best = 10000000
```
Теперь пробежим по колонкам
```python
for column in columns:
	current_columns.append(column)
	power +=1
	y = data['meter_reading']
	x = MimMaxScaler().fit_transform(data['current_columns'])
	model = LinearRegression().fit(x, y)
	bic = calculate_bic(y, model.prdict(x), power)
	if bic < big_best:
		best_columns.append(column)
		best_power +=1
		best_power = bic
	else:
		power +=1
		current_coluns.remove(column)
print(best_columns)
```

Для квадратичных значений
```python
for column in columns:
	data[column + '_2'] = np.multiply(data[column], data[column])
	
```
Цикл
```python
for column in columns:
	c = column + '_2'
	current_columns.append(c)
	power +=1
	y = data['meter_reading']
	x = MinMaxScaler().fit_transform(data[current_columns])
	bic = calculate_bic( y, LinearRegression().fit(x, y).predict(x), power)
	if bic < best_bic:
		best_columns.append(c)
		best_power +=1
		best_bic = bic
	else:
		power +=1
		current_columns.remove(c)
print(best_columns)
```
Для кубических значений
```python
for column in columns:
	data[column + '_3'] = np.multiply(data[column + '_2'], data[column])
```
Цикл
```python
for column in columns:
	c = columns + '_3'
	current_columns.append(c)
	power +=1
	x = MinMaxScaler().fit_transform(data['current_columns'])
	y = data['meter_reading']
	bic = calculate_bic(y, LinearRegression().fit(x, y).predict(x), power)
	if bic < bic_best:
		best_power +=1
		best_columns.append(c)
		bic_best = bic
	else:
		power +=1
		current_columns.remove(c)
print(best_columns)
```
Теперь нормализуем данные исходя из значений best_columns
```python
data_norm = pd.DataFrame(MinMaxScaler().fit_transform(data[best_columns]))
```
Разобьем данные на тестовую и проверочную выборки
```python
train, test, y_train, y_test = train_test_split(data_norm, data['meter_reading'], test_size = 0.2)
```

**Теперь модели линейной и изотонической регрессий**
Функция рассчета RMSLE
```python
y = data['meter_reading']
def rmsle_err(y, y_pred):
	return ((np.log(1 + y)-np.log(1 + y_pred))**2).mean()**0.5
```

Линейная регрессия
``` python
model1 = LinearRegression().fit(train, y_train)
print('RMSLE ошибка линейной регрессии :{0:.5}'.format(rmsle_err(y_train, model1.predict(train))))
```

Изотоническая регресссия
```python
model2 = IsotonicRegression(out_of_bounds='clip').fit(train[0], y_train)
print('RMSLE ошибка изотонической ргерессии :{0:.5}'.format(rmsle_err(y_train, model2.predict(train[0]))))
```

ElasticNet регрессия
```python
def objective(trial):
	alpha = trial.suggest_float('alpha', 1e-10, 1, log=True)
	l1_ratio = trial.suggest_float('l1_ratio', 1e-3, 1, log=True)
	regressor_obj = ElasticNet(alpha = alpha, l1_ratio = l1_ratio, max_iter = 10000, tol =1)
	regressor_obj.fit(train, y_train)
	y_pred = regressor.predict(train)
	return mean_squared_log_error(y, y_pred)
```

найдем оптимальные alpha и l1_ratio
```python
study = optuna.create_study()
study.optimize(objective, n_trials = 100)
print(study.best_params)
```

На основании оптимальных значений построим регрессию
```python
model3 = ElasticNet(alpha = study.best_params['alpha'], l1_ratio = study.best_params['l1_ratio']).fit(train, y_train)
print('RMSLE ошибка ElasticNet регрессии :{0:.5}'.format(rmsle_err(y_train, model3.predict(train))))
```

Итоговая модель регрессии будет
```python
def objective(trial):
	alpha = trial.suggest_float('alpha', 1e-10, 1, log = True)
	beta = trial.suggest_float('beta', 1e-10, 1, log = True)
	y_pred = (alpha*model1.predict(test) + beta*model2.predict(test[0]) + (1 - alpha - beta)*model3.predict(test))
	return mean_squared_log_error(test, y_pred)
```
Поиск оптимальных коэффициентов
```python
study = optuna.create_study()
study.optimize(objective, n_trials = 100)
```
Вывод
```python
y_pred1 = model1.prdict(data_norm)
y_pred2 = model2.predict(data_norm[0])
y_pred3 = model3.predict(data_norm)
y_pred = (study.best_params['alpha']*y_pred1 + study.best_params['beta']*y_pred2 +(1 - study.best_params['alpha'] - study.best_params['beta'])*y_pred3)
print('RMSLE линейной регрессии :{0:.5}'.format(rmsle_err(y, y_pred1)))
print('RMSLE изотонической регрессии :{0:.5}'.format(rmsle_err(y, y_pred2)))
print('RMSLE ElasticNet регрессии :{0:.5}'.format(rmsle_err(y, y_pred3)))
print('RMSLE ансамбля :{0:.5}'.format(rmsle_err(y, y_pred)))
```